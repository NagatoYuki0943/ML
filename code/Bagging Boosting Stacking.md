E(X) 数学期望 = 均值

D(X) 方差

- Bagging: 重复训练多个不稳定的独立模型(方差大)得到一个稳定的模型(方差小) ，训练时数据可以进行bagging采样。比如随即森林。降低方差。
- Boosting: 重复训练多个弱的模型(偏差大) ，组合成一个强的模型(偏差小) 降低偏差 要按照顺序训练多个模型，根据小模型误差调整接下来的模型，关注误差大的部分，比如AdaBoost，gradient Boost，XGBoost。降低偏差。
- Stacking 和 Bagging 很像把多个模型放在一起降低方差，不过小模型种类可以不同，不需要bagging数据采样，都在原始数据训练，将所有模型数据拼接起来，形成长为n的向量，再通过全连接层得到最终输出，或者通过平均得到终值。降低偏差。多层Stacking降低方差。
    可以使用多层Stacking，不过容易过拟合，所以可以使用k-fold bagging数据划分降低过拟合，既能降低方差又能降低偏差。



