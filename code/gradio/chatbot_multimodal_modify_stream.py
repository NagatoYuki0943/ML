# å¯¼å…¥å¿…è¦çš„åº“
import gradio as gr
import numpy as np
from typing import Generator, Sequence, Any
import threading
import time
from loguru import logger
from pathlib import Path


logger.info(f"gradio version: {gr.__version__}")
save_path = Path("upload")
save_path.mkdir(parents=True, exist_ok=True)


class InterFace:
    global_session_id: int = 0
    lock = threading.Lock()


enable_btn = gr.update(interactive=True)
disable_btn = gr.update(interactive=False)
btn = dict[str, Any]


def multimodal_chat(
    query: dict,
    history: Sequence | None = None,
    max_new_tokens: int = 1024,
    temperature: float = 0.8,
    top_p: float = 0.8,
    top_k: int = 40,
    state_session_id: int = 0,
) -> Generator[tuple[Sequence, btn, btn, btn], None, None]:
    logger.info(f"{state_session_id = }")

    history = [] if history is None else list(history)
    logger.debug(f"old history: {history}")

    logger.info(
        {
            "max_new_tokens": max_new_tokens,
            "temperature": temperature,
            "top_p": top_p,
            "top_k": top_k,
        }
    )

    logger.info(f"query: {query}")
    query_text = query.get("text", "").strip()
    qyery_files = query.get("files", [])
    logger.info(f"query_text: {query_text}")
    logger.info(f"query_files: {qyery_files}")
    if len(query_text) == 0 and len(qyery_files) == 0:
        logger.warning("query is None, return history")
        yield history, enable_btn, enable_btn, enable_btn
        return

    # å°†å›¾ç‰‡æ”¾å…¥å†å²è®°å½•ä¸­
    for file in qyery_files:
        history += [
            {
                "role": "user",
                "content": {
                    "type": "image_url",
                    # path for gradio
                    "path": file,
                    # useless for gradio
                    "image_url": {
                        "url": file,
                    },
                },
            }
        ]
    if query_text is not None and len(query_text) > 0:
        history += [
            {
                "role": "user",
                "content": query_text,
            }
        ]

    yield history, disable_btn, disable_btn, disable_btn

    time.sleep(1)
    number: np.ndarray = np.random.randint(1, 100, 20)
    logger.info(f"number: {number}")
    logger.info(f"new history: {history}")

    _history = history
    for i in range(len(number)):
        time.sleep(0.1)
        logger.info(f"number[{i}] = {number[i]}")
        _history = history + [
            {
                "role": "assistant",
                "content": str(number[: i + 1]),
            },
        ]
        yield (
            _history,
            disable_btn,
            disable_btn,
            disable_btn,
        )

    logger.info(f"new history: {_history}")
    yield _history, enable_btn, enable_btn, enable_btn


def regenerate(
    history: Sequence | None = None,
    max_new_tokens: int = 1024,
    temperature: float = 0.8,
    top_p: float = 0.8,
    top_k: int = 40,
    state_session_id: int = 0,
) -> Generator[tuple[Sequence, btn, btn, btn], None, None]:
    history = [] if history is None else list(history)
    logger.debug(f"old history: {history}")

    content = ""
    for message in history[::-1]:
        # æ— è®ºå¦‚ä½•éƒ½åˆ é™¤åé¢çš„å€¼
        history.pop()
        if message["role"] == "user":
            content = message["content"]
            break
    logger.debug(f"content: {content}")
    query = {}
    if isinstance(content, str):
        query["text"] = content.strip()
    elif isinstance(content, tuple) and len(content) > 0:
        query["files"] = [content[0]]

    if len(query.get("text", "")) > 0 or len(query.get("files", [])) > 0:
        yield from multimodal_chat(
            query,
            history,
            max_new_tokens,
            temperature,
            top_p,
            top_k,
            state_session_id,
        )
    else:
        logger.warning("no history, can't regenerate")
        yield history, enable_btn, enable_btn, enable_btn


def undo(query: dict, history: Sequence | None = None) -> tuple[str, Sequence]:
    """æ¢å¤åˆ°ä¸Šä¸€è½®å¯¹è¯"""
    history = [] if history is None else list(history)

    query = ""
    for message in history[::-1]:
        # æ— è®ºå¦‚ä½•éƒ½åˆ é™¤åé¢çš„å€¼
        history.pop()
        if message["role"] == "user":
            content = message["content"]
            if isinstance(content, str):
                query = content
            break

    return query, history


def main():
    block = gr.Blocks()
    with block as demo:
        state_session_id = gr.State(0)

        with gr.Row(equal_height=True):
            with gr.Column(scale=15):
                gr.Markdown("""<h1><center>ğŸ¦ Lobster</center></h1>""")
            # gr.Image(value=LOGO_PATH, scale=1, min_width=10,show_label=False, show_download_button=False)

        with gr.Row():
            with gr.Column(scale=4):
                with gr.Row():
                    # åˆ›å»ºèŠå¤©æ¡†
                    chatbot = gr.Chatbot(
                        type="messages",
                        height=500,
                        show_copy_button=True,
                        placeholder="å†…å®¹ç”± AI å¤§æ¨¡å‹ç”Ÿæˆï¼Œè¯·ä»”ç»†ç”„åˆ«ã€‚",
                    )

                # ç»„å†…çš„ç»„ä»¶æ²¡æœ‰é—´è·
                with gr.Group():
                    with gr.Row():
                        # åˆ›å»ºä¸€ä¸ªæ–‡æœ¬æ¡†ç»„ä»¶ï¼Œç”¨äºè¾“å…¥ promptã€‚
                        query = gr.MultimodalTextbox(
                            file_types=["image"],
                            file_count="multiple",  # æŒ‡çš„æ˜¯ä¸€æ¬¡ä¸Šä¼ å‡ å¼ ,é€‰æ‹©singleä¹Ÿå¯ä»¥å¤šæ¬¡é€‰æ‹©
                            placeholder="Enter å‘é€; Shift + Enter æ¢è¡Œ / Enter to send; Shift + Enter to wrap",
                            label="Prompt / é—®é¢˜",
                            interactive=True,
                        )

                with gr.Row():
                    # åˆ›å»ºä¸€ä¸ªé‡æ–°ç”ŸæˆæŒ‰é’®ï¼Œç”¨äºé‡æ–°ç”Ÿæˆå½“å‰å¯¹è¯å†…å®¹ã€‚
                    retry_btn = gr.Button("ğŸ”„ Retry", variant="secondary")
                    undo_btn = gr.Button("â†©ï¸ Undo", variant="secondary")
                    # åˆ›å»ºä¸€ä¸ªæ¸…é™¤æŒ‰é’®ï¼Œç”¨äºæ¸…é™¤èŠå¤©æœºå™¨äººç»„ä»¶çš„å†…å®¹ã€‚
                    clear_btn = gr.ClearButton(
                        components=[chatbot, query], value="ğŸ—‘ï¸ Clear", variant="stop"
                    )

                # æŠ˜å 
                with gr.Accordion("Advanced Options", open=False):
                    with gr.Row():
                        max_new_tokens = gr.Slider(
                            minimum=1,
                            maximum=2048,
                            value=1024,
                            step=1,
                            label="Max new tokens",
                        )
                        temperature = gr.Slider(
                            minimum=0.01,
                            maximum=2,
                            value=0.8,
                            step=0.01,
                            label="Temperature",
                        )
                        top_p = gr.Slider(
                            minimum=0.01, maximum=1, value=0.8, step=0.01, label="Top_p"
                        )
                        top_k = gr.Slider(
                            minimum=1, maximum=100, value=40, step=1, label="Top_k"
                        )

                gr.Examples(
                    examples=[
                        {"text": "ä½ æ˜¯è°", "files": []},
                        {
                            "text": "è¿™å¼ å›¾ç‰‡å±•ç¤ºçš„ä»€ä¹ˆå†…å®¹?",
                            "files": ["images/0001.jpg"],
                        },
                        {
                            "text": "è¿™2å¼ å›¾ç‰‡å±•ç¤ºçš„ä»€ä¹ˆå†…å®¹?",
                            "files": ["images/0001.jpg", "images/0002.jpg"],
                        },
                    ],
                    inputs=[query],
                    label="ç¤ºä¾‹é—®é¢˜ / Example questions",
                )

            # å›è½¦æäº¤(æ— æ³•ç¦æ­¢æŒ‰é’®)
            query.submit(
                multimodal_chat,
                inputs=[
                    query,
                    chatbot,
                    max_new_tokens,
                    temperature,
                    top_p,
                    top_k,
                    state_session_id,
                ],
                outputs=[chatbot, retry_btn, undo_btn, clear_btn],
            )

            # æ¸…ç©ºquery
            query.submit(
                lambda: gr.MultimodalTextbox(value={"text": "", "files": []}),
                inputs=[],
                outputs=[query],
            )

            # é‡æ–°ç”Ÿæˆ
            retry_btn.click(
                regenerate,
                inputs=[
                    chatbot,
                    max_new_tokens,
                    temperature,
                    top_p,
                    top_k,
                    state_session_id,
                ],
                outputs=[chatbot, retry_btn, undo_btn, clear_btn],
            )

            # æ’¤é”€
            undo_btn.click(undo, inputs=[query, chatbot], outputs=[query, chatbot])

        gr.Markdown("""æé†’ï¼š<br>
        1. å†…å®¹ç”± AI å¤§æ¨¡å‹ç”Ÿæˆï¼Œè¯·ä»”ç»†ç”„åˆ«ã€‚<br>
        """)

        # åˆå§‹åŒ–session_id
        def init():
            with InterFace.lock:
                InterFace.global_session_id += 1
            new_session_id = InterFace.global_session_id
            return new_session_id

        demo.load(init, inputs=None, outputs=[state_session_id])

    # threads to consume the request
    gr.close_all()

    # è®¾ç½®é˜Ÿåˆ—å¯åŠ¨
    demo.queue(
        max_size=None,  # If None, the queue size will be unlimited.
        default_concurrency_limit=100,  # æœ€å¤§å¹¶å‘é™åˆ¶
    )

    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=True,
        max_threads=100,
    )


if __name__ == "__main__":
    main()
